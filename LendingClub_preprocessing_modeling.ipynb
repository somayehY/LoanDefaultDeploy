{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad5cebe",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [1. Background](#Background)\n",
    "* [2. Imports](#import)\n",
    "* [3. Load the data](#Load_Data)\n",
    "* [4. Is the data imbalanced?](#imblnc)\n",
    "* [5. Data preparation prior to model training](#pre)\n",
    "* [6. Choose the right metrics for model evaluation](#metrics)\n",
    "* [7. Train models](#model)\n",
    "    * [7.1. Logistic regression](#lr)\n",
    "        * [7.1.1.  Make a pipeline for logistic regression model training](#pllr)\n",
    "        * [7.1.2. Hyperparameter search using GridSearchCV for logistic regression](#gdlr)\n",
    "        * [7.1.3. Best Logistic Regression model](#bestlr)\n",
    "    * [7.2. Decision tree classifier](#dt)\n",
    "        * [7.2.1. Make a pipeline for decision tree](#pldt)\n",
    "        * [7.2.2. Hyperparameter search for decision tree classifier](#gddt)\n",
    "        * [7.2.3. Decision tree classifier with best parameters](#bestdt)\n",
    "        * [7.2.4. Feature importance assessment in the decision tree classifier](#fidt)\n",
    "    * [7.3. Random forest classifier](#rf)\n",
    "        * [7.3.1. Make a pipeline for random forest classifier](#plrf)\n",
    "        * [7.3.2. Hyperparameter search for random forest classifier](#gdrf)\n",
    "        * [7.3.3. Rondom forest classifier feature importance assessment](#firf)\n",
    "    * [7.4. Gradient boosting classifier](#GB)\n",
    "        * [7.4.1. Make a pipeline for gradient boosting classifier](#plgb)\n",
    "        * [7.4.2. Hyperparameter tuning for gradient boosting classifier](#gdgb)\n",
    "        * [7.4.3. Gradient boosting feature importance assessment](#figb)\n",
    "* [8. Final model selection](#select)\n",
    "    * [8.1. Logistic regressio](#lr_test)\n",
    "    * [8.2. Decision tree](#dt_test)\n",
    "    * [8.3 Random forest](#rf_test)\n",
    "    * [8.4. XGBoost](#XG_test)\n",
    "    * [8.5. Discussion](#disc)\n",
    "* [9. Save the best model](#save_model)\n",
    "* [10. Summary](#discussion)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad8133",
   "metadata": {},
   "source": [
    "# 1. Background  <a class='anchor' id='Background'></a>\n",
    "\n",
    "In this notebook we will apply several calssification models to our cleaned data frame of accepted loans from Lending Club. Briefly, Lending Club used to be the biggest peer to peer lending platform. To decide about a loan application, Lending Club relies on applicants' information provided during application. Such information includes income, length employment and credit history. In previous notebooks, we addressed missing data and explore the data to get a better understanding of the data \n",
    "\n",
    "In this notebook we will apply several classification models to predict if a loan will default. The models are:\n",
    "\n",
    "1. Logistic regression\n",
    "\n",
    "2. Decision tree\n",
    "\n",
    "3. Random Forest\n",
    "\n",
    "4. Gradient boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104aefea",
   "metadata": {},
   "source": [
    "# 2. Imports <a class='anchor' id='import'></a>\n",
    "\n",
    "We start by importing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81d33209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn import tree, metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_recall_curve, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffb59e",
   "metadata": {},
   "source": [
    "# 3. Load the data<a class='anchor' id='Load_Data'></a>\n",
    "\n",
    "Using the pd.read_csv, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb32bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\somfl\\\\Documents\\\\Data Science Career Track\\\\LendingClub\\\\LendingClubClean.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a3cbc",
   "metadata": {},
   "source": [
    "# 4. Is the data imbalanced? <a class='anchor' id='imblnc'><a/>\n",
    "\n",
    "A data set is called [imbalanced](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data), if the minority class makes a small percentage of the data set. \n",
    "\n",
    "If the minority class (in our case the defaulted loans) makes 20 to 40% of a data set, then it is mildly imbalance.\n",
    "if the minority class is 1 to 20%, then the dataset is moderately imbalanced and if minority class is < 1% of data set, the data is extremely imbalanced.\n",
    "\n",
    "To find out if our data set is imbalanced or not, we will look at waht percentages of loan applications in the data frame is defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76cb9bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid    0.783049\n",
       "Default       0.216951\n",
       "Name: loan_status, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loan_status.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d5290",
   "metadata": {},
   "source": [
    "Default loans count for 22% of our data set. Therefore, our data is mildly imbalance, which may or may not be a problem. It is suggested to model with the true distribution and if it was not fine, apply techncis such as undersampling to deal with the imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443df4d",
   "metadata": {},
   "source": [
    "# 5. Data preparation prior to model training <a class='anchor' id='pre'><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98540a9",
   "metadata": {},
   "source": [
    "Due to complexity of our data frame, we definde two functions to handle all the prerocessing steps we need to do prior to training. These steps are:\n",
    "\n",
    "1. Defining X and y\n",
    "2. Applying One Hot Encoder to change categorical data\n",
    "3. Under sampling the data by using RandomUnderSampler\n",
    "4. spliting the data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "687685e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revol_util                      880\n",
       "dti                               0\n",
       "chargeoff_within_12_mths          0\n",
       "collections_12_mths_ex_med        0\n",
       "inq_last_6mths                   30\n",
       "open_acc                         29\n",
       "mort_acc                      50030\n",
       "annual_inc                        4\n",
       "sub_grade                         0\n",
       "loan_status                       0\n",
       "installment                       0\n",
       "int_rate                          0\n",
       "term                              0\n",
       "revol_bal                         0\n",
       "emp_length                    77070\n",
       "home_ownership                    0\n",
       "num_rev_accts                 70277\n",
       "pub_rec_bankruptcies              0\n",
       "tax_liens                         0\n",
       "loan_amnt                         0\n",
       "Credit Length (year)             29\n",
       "fico_score                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6c80a",
   "metadata": {},
   "source": [
    "## Model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5386c852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(transformers=[('cat',\n",
       "                                                                         Pipeline(steps=[('cat_impute',\n",
       "                                                                                          SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                         ('encoder',\n",
       "                                                                                          OneHotEncoder(drop='if_binary'))]),\n",
       "                                                                         ['term']),\n",
       "                                                                        ('num',\n",
       "                                                                         Pipeline(steps=[('num_impute',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('scaler',\n",
       "                                                                                          MinMaxScaler())]),\n",
       "                                                                         ['int_rate',\n",
       "                                                                          'loan_amnt',\n",
       "                                                                          'fico_score'])])),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.1, 100],\n",
       "                         'classifier__penalty': ['l1', 'l2']},\n",
       "             scoring='balanced_accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['term']\n",
    "\n",
    "numerical_columns = ['int_rate', 'loan_amnt', 'fico_score']\n",
    "yy = df['loan_status']\n",
    "XX = df[numerical_columns + categorical_columns]\n",
    "\n",
    "X_cols = XX.columns\n",
    "y_col = ['status']\n",
    "\n",
    "Xn = XX.to_numpy()\n",
    "yn = yy.to_numpy()\n",
    "\n",
    "RU = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = RU.fit_resample(Xn,yn)\n",
    "\n",
    "X = pd.DataFrame(X_res, columns=X_cols)\n",
    "y = pd.DataFrame(y_res, columns=y_col)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[('cat_impute', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(drop='if_binary'))])\n",
    "numerical_pipe = Pipeline(steps=[('num_impute', SimpleImputer(strategy='median')), ('scaler', MinMaxScaler())])\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", categorical_pipe, categorical_columns),\n",
    "        (\"num\", numerical_pipe, numerical_columns),\n",
    "    ]\n",
    ")\n",
    "lr= Pipeline(\n",
    "    [\n",
    "        (\"preprocess\", preprocessing),\n",
    "        (\"classifier\", LogisticRegression(random_state=42))\n",
    "    ])\n",
    "\n",
    "grid_params = {'classifier__penalty': ['l1','l2'],\n",
    "               'classifier__C': [0.1,100]\n",
    "              }\n",
    "  \n",
    "\n",
    "grid_cv = GridSearchCV(lr, param_grid=grid_params, cv=5, scoring = 'balanced_accuracy', n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e75dc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_cv.best_estimator_.predict(X_test)\n",
    "y_prob = grid_cv.best_estimator_.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a32490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model =grid_cv.best_estimator_\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91b54b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1312998602.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [44]\u001b[1;36m\u001b[0m\n\u001b[1;33m    '\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "with open(os.path.join('C:\\\\Users\\\\somfl\\\\Documents\\\\GitHub\\\\LoanDefaultDeploy','Procfile'), \"w\") as file1:\n",
    "    toFile = 'web: sh setup.sh && streamlit run test.py'\n",
    "    'web: sh setup.sh && streamlit run test.py'\n",
    "'\n",
    "    \n",
    "file1.write('web: sh setup.sh && streamlit run <app name>.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
